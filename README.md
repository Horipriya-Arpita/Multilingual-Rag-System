# ðŸ¤– Multilingual RAG System (Bangla + English)

A simple **Multilingual Retrieval-Augmented Generation (RAG)** system built to answer both **Bengali and English** queries from a scanned Bengali textbook (HSC Bangla 1st Paper). It uses **OCR**, **vector search**, and **LLM-based response generation** to answer questions from documents.

---

## ðŸ“Œ Features

- ðŸ” Accepts Bangla and English questions.
- ðŸ“š Extracts content from scanned Bengali PDF using OCR.
- ðŸ§¹ Cleans and chunks the text with paragraph-aware logic.
- ðŸ§  Embeds and indexes with multilingual sentence transformers.
- ðŸ¤– Answers questions with context using **Gemini** (Google Generative AI).
- ðŸ”— Includes REST API using FastAPI.
- ðŸ§ª Includes manual evaluation + question relevance tracking.

---

## ðŸ“ Folder Structure

```
Multilingual-RAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py               # OCR Extraction (Tesseract)
â”‚   â”œâ”€â”€ preprocess.py         # Cleaning
â”‚   â”œâ”€â”€ embed_chunks.py       # Chunking + FAISS
â”‚   â”œâ”€â”€ rag.py                # Core Retrieval + LLM logic
â”‚   â”œâ”€â”€ api.py                # FastAPI endpoint
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hsc.pdf               # (Bengali book - not uploaded)
â”‚   â”œâ”€â”€ processed_text.txt
â”‚   â”œâ”€â”€ cleaned_text.txt
â”‚   â”œâ”€â”€ sample_queries.md
â”‚   â””â”€â”€ faiss/
â”‚       â”œâ”€â”€ index.bin
â”‚       â””â”€â”€ chunks.pkl
â”œâ”€â”€ .env.example              # Place your GEMINI_API_KEY here
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ project_answers.md
â”œâ”€â”€ evaluation.md
```

---

## ðŸš€ Setup Instructions

### 1ï¸âƒ£ Clone & Setup Environment

```bash
git clone https://github.com/your-username/Multilingual-RAG.git
cd Multilingual-RAG

python -m venv venv
venv\Scripts\activate   # Windows
# source venv/bin/activate  # Linux/Mac

pip install -r requirements.txt
```

### 2ï¸âƒ£ Add Gemini API Key

Create `.env` file:

```env
GEMINI_API_KEY=your_api_key_here
```

### 3ï¸âƒ£ Run Preprocessing Steps

```bash
python src/main.py          # OCR
python src/preprocess.py    # Clean text
python src/embed_chunks.py  # Chunk & embed text
```

---

## ðŸ§ª Run the System

### CLI Interface
```bash
python src/rag.py
```

### API Interface
```bash
uvicorn src.api:app --reload
```

---

## ðŸ”— API Documentation

**POST** `/ask`

**Request Body:**
```json
{
  "query": "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?"
}
```

**Response:**
```json
{
  "query": "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?",
  "answer": "à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥"
}
```

---

## ðŸ“ Sample Queries

| User Question | Expected Answer |
|---------------|-----------------|
| à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡? | à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥ |
| à¦•à¦¾à¦•à§‡ à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ à¦¬à¦²à§‡ à¦‰à¦²à§à¦²à§‡à¦– à¦•à¦°à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡? | à¦®à¦¾à¦®à¦¾à¦•à§‡ |
| à¦¬à¦¿à¦¯à¦¼à§‡à¦° à¦¸à¦®à¦¯à¦¼ à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦ªà§à¦°à¦•à§ƒà¦¤ à¦¬à¦¯à¦¼à¦¸ à¦•à¦¤ à¦›à¦¿à¦²? | à§§à§« à¦¬à¦›à¦° |

---

## âœ… Evaluation Summary

| Question | Retrieved Context Correct? | Answer Match | Comments |
|----------|----------------------------|--------------|----------|
| à¦¸à§à¦ªà§à¦°à§à¦· | âœ… | âœ… | Exact match |
| à¦­à¦¾à¦—à§à¦¯ à¦¦à§‡à¦¬à¦¤à¦¾ | âœ… | âœ… | Match |
| à¦•à¦²à§à¦¯à¦¾à¦£à§€à¦° à¦¬à¦¯à¦¼à¦¸ | âœ… | âœ… | Accurate |

---

## â“ Answers to Assignment Questions

### Q1: What method/library did you use to extract the text, and why?

> I used `pytesseract` with `pdf2image` to handle the scanned PDF. Since it's an image-based document (not digital text), OCR is necessary. Tesseract with the Bengali language pack (`lang='ben'`) provided decent accuracy. Preprocessing was required to fix common OCR issues.

---

### Q2: What chunking strategy did you choose?

> I used `RecursiveCharacterTextSplitter` with `chunk_size=300` and `chunk_overlap=50`, splitting on Bengali-specific punctuation like `à¥¤`, as well as `\n`. This gives semantic chunks while keeping the FAISS vector space effective.

---

### Q3: What embedding model did you use and why?

> I used `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, which supports over 50 languages including Bangla. It provides strong semantic embeddings and is lightweight enough for local experimentation.

---

### Q4: How do you compare query with stored chunks?

> I use **FAISS** with L2 distance to find top-k most similar vectors. The same embedding model is used for both queries and document chunks to ensure uniform semantic space.

---

### Q5: How do you ensure meaningful comparison?

> Since both the query and chunks are embedded using the same multilingual transformer, they map to a shared latent space. If a query is vague, the system may retrieve general or unrelated results â€” better chunking or larger corpus could help mitigate that.

---

### Q6: Do the results seem relevant? If not, how to improve?

> Yes, the results are generally relevant and grounded. However, I believe they could be even better if I had access to OpenAI models â€” I couldnâ€™t use them due to a lack of credits/resources. If I could work with GPT-4 or similar, the output would likely be more accurate and insightful.  
Additionally, I wasnâ€™t able to use LangChain fully due to time and resource limitations. If I had used it, the system could have benefited from better document management, chunking strategies, and prompt optimization.  
Future improvements could include:
> - Sentence-level chunking for cleaner context
> - Larger context windows in prompts  
> - Using advanced models like Gemini Pro or GPT-4 
> - Associating metadata with chunks for better filtering 

---

## ðŸ› ï¸ Tools & Libraries Used

- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
- [pdf2image](https://pypi.org/project/pdf2image/)
- [LangChain](https://www.langchain.com/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Sentence-Transformers](https://www.sbert.net/)
- [Gemini API (Google Generative AI)](https://ai.google.dev/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Python-dotenv](https://pypi.org/project/python-dotenv/)
- [langdetect](https://pypi.org/project/langdetect/)

---

## âœ… Final Notes

- Works on scanned Bengali PDFs and supports multilingual queries
- Clean, modular code with API integration
- Easy to extend with frontend or chatbot interface

---

> Built for the **AI Engineer (Level-1) Technical Assessment**  
> Author: Horipriya Das Arpita   
> Contact: horipriya288@gmail.com  
> Github: https://github.com/Horipriya-Arpita  
